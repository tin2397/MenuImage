{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjNsE-YjbCew"
      },
      "source": [
        "# Mass Embedding of Bioacoustic Audio\n",
        "\n",
        "This notebook facilitates pre-computing embeddings of audio data for subsequent\n",
        "use with search, classification, and analysis.\n",
        "\n",
        "# ATTENTION:\n",
        "\n",
        "There is a new version of this workflow avialable [here](https://github.com/google-research/perch-hoplite/blob/main/perch_hoplite/agile/1_embed_audio_v2.ipynb), in the new [Perch-Hoplite](https://github.com/google-research/perch-hoplite/blob/main/perch_hoplite) respository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avlqyEzpa_rN"
      },
      "source": [
        "## Configuration and Imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPdB8b3qAb6I",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title Installation. { vertical-output: true }\n",
        "#@markdown Run this notebook in Google Colab by following [this link](https://colab.research.google.com/github/google-research/perch/blob/main/embed_audio.ipynb).\n",
        "#@markdown\n",
        "#@markdown Run this cell to install the project dependencies.\n",
        "%pip install git+https://github.com/google-research/perch.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kuA7b5Wap7o"
      },
      "outputs": [],
      "source": [
        "#@title Imports. { vertical-output: true }\n",
        "\n",
        "from etils import epath\n",
        "from ml_collections import config_dict\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "from chirp.inference import colab_utils\n",
        "colab_utils.initialize(use_tf_gpu=True, disable_warnings=True)\n",
        "\n",
        "from chirp import audio_utils\n",
        "from chirp.inference import embed_lib\n",
        "from chirp.inference import tf_examples\n",
        "from perch_hoplite.zoo import model_configs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-l4NL0CAavKl"
      },
      "outputs": [],
      "source": [
        "#@title Basic Configuration. { vertical-output: true }\n",
        "\n",
        "#@markdown Define the model: perch or birdnet are most common for birds.\n",
        "model_choice = 'perch_8'  #@param['perch_8', 'humpback', 'multispecies_whale', 'surfperch', 'birdnet_V2.3']\n",
        "#@markdown Set the base directory for the project.\n",
        "working_dir = '/content/test_solo'  #@param\n",
        "\n",
        "# Set the embedding and labeled data directories.\n",
        "embeddings_path = epath.Path(working_dir) / 'embeddings'\n",
        "labeled_data_path = epath.Path(working_dir) / 'labeled'\n",
        "embeddings_glob = embeddings_path / 'embeddings-*'\n",
        "\n",
        "# OPTIONAL: Set up separation model.\n",
        "separation_model_key = 'separator_model_tf'  #@param\n",
        "separation_model_path = ''  #@param\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7W8Rl0ma8Mm"
      },
      "source": [
        "## Embed Audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vobyRomeazNr"
      },
      "outputs": [],
      "source": [
        "#@title Embedding Configuration. { vertical-output: true }\n",
        "\n",
        "config = config_dict.ConfigDict()\n",
        "config.embed_fn_config = config_dict.ConfigDict()\n",
        "config.embed_fn_config.model_config = config_dict.ConfigDict()\n",
        "\n",
        "#@markdown IMPORTANT: Select the target audio files.\n",
        "#@markdown source_file_patterns should contain a list of globs of audio files, like:\n",
        "#@markdown ['/home/me/*.wav', '/home/me/other/*.flac']\n",
        "config.source_file_patterns = ['/content/drive/MyDrive/google COLAB/Penguin/Archive/13B3-1_clipped/13B3-1_exhale-i10_10.wav']  #@param\n",
        "config.output_dir = embeddings_path.as_posix()\n",
        "\n",
        "preset_info = model_configs.get_preset_model_config(model_choice)\n",
        "config.embed_fn_config.model_key = preset_info.model_key\n",
        "config.embed_fn_config.model_config = preset_info.model_config\n",
        "\n",
        "# Only write embeddings to reduce size.\n",
        "config.embed_fn_config.write_embeddings = True\n",
        "config.embed_fn_config.write_logits = True\n",
        "config.embed_fn_config.write_separated_audio = False\n",
        "config.embed_fn_config.write_raw_audio = False\n",
        "\n",
        "#@markdown File sharding automatically splits audio files into one-minute chunks\n",
        "#@markdown for embedding. This limits both system and GPU memory usage,\n",
        "#@markdown especially useful when working with long files (>1 hour).\n",
        "use_file_sharding = True  #@param {type:'boolean'}\n",
        "if use_file_sharding:\n",
        "  config.shard_len_s = 60.0\n",
        "\n",
        "# Number of parent directories to include in the filename.\n",
        "config.embed_fn_config.file_id_depth = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiWVT22ja1Y0"
      },
      "outputs": [],
      "source": [
        "#@title Set up. { vertical-output: true }\n",
        "\n",
        "# Set up the embedding function, including loading models.\n",
        "embed_fn = embed_lib.EmbedFn(**config.embed_fn_config)\n",
        "print('\\n\\nLoading model(s)...')\n",
        "embed_fn.setup()\n",
        "\n",
        "# Create output directory and write the configuration.\n",
        "output_dir = epath.Path(config.output_dir)\n",
        "output_dir.mkdir(exist_ok=True, parents=True)\n",
        "embed_lib.maybe_write_config(config, output_dir)\n",
        "\n",
        "# Create SourceInfos.\n",
        "source_infos = embed_lib.create_source_infos(\n",
        "    config.source_file_patterns,\n",
        "    num_shards_per_file=config.get('num_shards_per_file', -1),\n",
        "    shard_len_s=config.get('shard_len_s', -1))\n",
        "print(f'Found {len(source_infos)} source infos.')\n",
        "\n",
        "print('\\n\\nTest-run of model...')\n",
        "window_size_s = config.embed_fn_config.model_config.window_size_s\n",
        "sr = config.embed_fn_config.model_config.sample_rate\n",
        "z = np.zeros([int(sr * window_size_s)], dtype=np.float32)\n",
        "embed_fn.embedding_model.embed(z)\n",
        "print('Setup complete!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jf8RVwRwa350",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title Run embedding. { vertical-output: true }\n",
        "\n",
        "# Uses multiple threads to load audio before embedding.\n",
        "# This tends to be faster, but can fail if any audio files are corrupt.\n",
        "\n",
        "embed_fn.min_audio_s = 1.0\n",
        "record_file = (output_dir / 'embeddings.tfrecord').as_posix()\n",
        "succ, fail = 0, 0\n",
        "\n",
        "existing_embedding_ids = embed_lib.get_existing_source_ids(\n",
        "    output_dir, 'embeddings-*')\n",
        "\n",
        "new_source_infos = embed_lib.get_new_source_infos(\n",
        "    source_infos, existing_embedding_ids, config.embed_fn_config.file_id_depth)\n",
        "\n",
        "print(f'Found {len(existing_embedding_ids)} existing embedding ids. \\n'\n",
        "      f'Processing {len(new_source_infos)} new source infos. ')\n",
        "\n",
        "try:\n",
        "  audio_loader = lambda fp, offset: audio_utils.load_audio_window(\n",
        "      fp, offset, sample_rate=config.embed_fn_config.model_config.sample_rate,\n",
        "      window_size_s=config.get('shard_len_s', -1.0))\n",
        "  audio_iterator = audio_utils.multi_load_audio_window(\n",
        "      filepaths=[s.filepath for s in new_source_infos],\n",
        "      offsets=[s.shard_num * s.shard_len_s for s in new_source_infos],\n",
        "      audio_loader=audio_loader,\n",
        "  )\n",
        "  with tf_examples.EmbeddingsTFRecordMultiWriter(\n",
        "      output_dir=output_dir, num_files=config.get('tf_record_shards', 1)) as file_writer:\n",
        "    for source_info, audio in tqdm.tqdm(\n",
        "        zip(new_source_infos, audio_iterator), total=len(new_source_infos)):\n",
        "      if not embed_fn.validate_audio(source_info, audio):\n",
        "        continue\n",
        "      file_id = source_info.file_id(config.embed_fn_config.file_id_depth)\n",
        "      offset_s = source_info.shard_num * source_info.shard_len_s\n",
        "      example = embed_fn.audio_to_example(file_id, offset_s, audio)\n",
        "      if example is None:\n",
        "        fail += 1\n",
        "        continue\n",
        "      file_writer.write(example.SerializeToString())\n",
        "      succ += 1\n",
        "    file_writer.flush()\n",
        "finally:\n",
        "  del(audio_iterator)\n",
        "print(f'\\n\\nSuccessfully processed {succ} source_infos, failed {fail} times.')\n",
        "\n",
        "fns = [fn for fn in output_dir.glob('embeddings-*')]\n",
        "ds = tf.data.TFRecordDataset(fns)\n",
        "parser = tf_examples.get_example_parser()\n",
        "ds = ds.map(parser)\n",
        "for ex in ds.as_numpy_iterator():\n",
        "  print(ex['filename'])\n",
        "  print(ex['embedding'].shape, flush=True)\n",
        "  break\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "# PATH\n",
        "audio_path = '/content/drive/MyDrive/google COLAB/Penguin/Archive/14B19-1_clipped/14B19-1_exhale-i100_153.wav'\n",
        "\n",
        "# Modify sample rate\n",
        "audio, sample_rate = librosa.load(audio_path, sr=32000)\n",
        "\n",
        "# Input the audio to Perch\n",
        "outputs = embed_fn.embedding_model.embed(audio)\n",
        "# Inspect\n",
        "print(\"\\n--- INSPECTING THE OUTPUT ---\")\n",
        "\n",
        "# Inspecting the Embeddings\n",
        "# how Perch translates a 5s audio into a mathematical format\n",
        "print(f\"Embeddings Shape: {outputs.embeddings.shape}\")\n",
        "\n",
        "# Inspecting the Logits\n",
        "# Represents the model's attempt to guess what bird it just heard\n",
        "print(f\"Logits Shape:     {outputs.logits['label'].shape}\")\n",
        "\n",
        "# Calculate the final classification by finding the highest logit score\n",
        "mean_logits = np.mean(outputs.logits['label'], axis=0)\n",
        "top_prediction_index = np.argmax(mean_logits)\n",
        "\n",
        "print(f\"\\nPerch's Top Prediction (Class ID): {top_prediction_index}\")"
      ],
      "metadata": {
        "id": "WhnuJuldxKlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import librosa\n",
        "from scipy.special import softmax\n",
        "\n",
        "def analyze_top_frequency(folder_path, penguin_name):\n",
        "    files = tf.io.gfile.glob(f\"{folder_path}/*.wav\")\n",
        "    all_top_indices = []\n",
        "    all_confidences = []\n",
        "\n",
        "    print(f\"Scanning {len(files)} files for {penguin_name}...\")\n",
        "\n",
        "    for f in files:\n",
        "        audio, _ = librosa.load(f, sr=32000)\n",
        "        out = embed_fn.embedding_model.embed(audio)\n",
        "\n",
        "        # Get probabilities via Softmax\n",
        "        raw_logits = np.mean(out.logits['label'], axis=0)\n",
        "        probs = softmax(raw_logits)\n",
        "\n",
        "        # Track the #1 guess for this file\n",
        "        top_idx = np.argmax(probs)\n",
        "        all_top_indices.append(top_idx)\n",
        "        all_confidences.append(probs[top_idx])\n",
        "\n",
        "    # Find the Most Frequent ID\n",
        "    counter = collections.Counter(all_top_indices)\n",
        "    most_common_id, frequency = counter.most_common(1)[0]\n",
        "\n",
        "    # Get the average confidence for that specific ID\n",
        "    # We only average the confidence scores when the model picked the winning ID\n",
        "    winning_confidences = [c for i, c in zip(all_top_indices, all_confidences) if i == most_common_id]\n",
        "    avg_winning_conf = np.mean(winning_confidences) * 100\n",
        "\n",
        "    print(f\"\\n--- {penguin_name} MAJORITY ANALYSIS ---\")\n",
        "    print(f\"TOP FREQUENT ID:    {most_common_id}\")\n",
        "    print(f\"REPETITION RATE:    {frequency}/{len(files)} files ({(frequency/len(files)*100):.2f}%)\")\n",
        "    print(f\"AVG CONFIDENCE:     {avg_winning_conf:.2f}%\")\n",
        "    print(\"--------\")\n",
        "\n",
        "    return most_common_id, avg_winning_conf\n",
        "\n",
        "# Run for both penguins\n",
        "id_13b3, conf_13b3 = analyze_top_frequency('/content/drive/MyDrive/google COLAB/Penguin/Archive/13B3-1_clipped', '13B3-1')\n",
        "id_14b19, conf_14b19 = analyze_top_frequency('/content/drive/MyDrive/google COLAB/Penguin/Archive/14B19-1_clipped', '14B19-1')"
      ],
      "metadata": {
        "id": "IC-OrxhmP8Tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CVL3zfEmy9h"
      },
      "outputs": [],
      "source": [
        "from chirp.inference import tf_examples\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = [] # Store input data\n",
        "y = [] #Store labels"
      ],
      "metadata": {
        "id": "VNwVVT-SbPBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_extract(folder_path, penguin_label):\n",
        "    # Retrieve all embedded files\n",
        "    fns = tf.io.gfile.glob(f\"{folder_path}/embeddings-*\")\n",
        "\n",
        "    # Use TFRecordDataset for memory-efficient data loading\n",
        "    # as loading embeddings into RAM all at once can cause crashes\n",
        "    ds = tf.data.TFRecordDataset(fns)\n",
        "    parser = tf_examples.get_example_parser()\n",
        "    ds = ds.map(parser)\n",
        "\n",
        "    for ex in ds.as_numpy_iterator():\n",
        "        # Perch outputs high-dimensional 3D embeddings (Time, Channel, Features)\n",
        "        emb = ex['embedding']\n",
        "\n",
        "        # Average all the data across axis 0 (Time) and axis 1 (Channel), but leave axis 2 (the 1,280 features) alone\n",
        "        mean_emb = np.mean(emb, axis=(0, 1))\n",
        "        # Store the flattened features (X) and the corresponding ground truth label (y)\n",
        "        X.append(mean_emb)\n",
        "        y.append(penguin_label)\n",
        "\n",
        "\n",
        "# Assigning labels: 0 for 13B3-1 and 1 for 14B19-1.\n",
        "# Binary classification\n",
        "\n",
        "# Point these to the output_dir folders\n",
        "print(\"Loading Penguin 13B3-1 data...\")\n",
        "load_and_extract('/content/drive/MyDrive/google COLAB/RESULT/PEN_13B3/embeddings', 0)\n",
        "\n",
        "print(\"Loading Penguin 14B19-1 data...\")\n",
        "load_and_extract('/content/drive/MyDrive/google COLAB/RESULT/PEN_14B19/embeddings', 1)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Check if the audio loaded or not\n",
        "print(f\"Total dataset ready: {X.shape[0]} audio windows loaded.\")\n",
        "\n",
        "# Split the data\n",
        "# Using a 75/25 split to ensure the model is evaluated on 'unseen' data\n",
        "# Note: X_train == set of 1,280 features the Random Forest uses to study\n",
        "# y_train: labels (0 or 1) that tell the model which penguin is speaking in X_train\n",
        "# X_test: 25% data the model haven't seen\n",
        "# y_test: labels for X_test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Train the Random Forest\n",
        "print(\"\\nTraining Random Forest Classifier...\")\n",
        "# n_estimators=100 == 100 individual decision trees\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the Custom Model\n",
        "# Given 25% test data\n",
        "rf_accuracy = accuracy_score(y_test, rf_model.predict(X_test))\n",
        "\n",
        "\n",
        "\n",
        "# --- FINAL COMPARISON---\n",
        "print(\"\\n--- RESULTS ---\")\n",
        "print(f\"Random Forest Accuracy: {rf_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "ueSwk-ERcCCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "OsKVsuD3jlJB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "runtime_attributes": {
        "runtime_version": "2025.07"
      }
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}